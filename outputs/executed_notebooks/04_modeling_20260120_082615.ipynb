{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4359aeee",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [1]</a>'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f273e3",
   "metadata": {
    "papermill": {
     "duration": 0.008382,
     "end_time": "2026-01-20T01:26:17.558865",
     "exception": false,
     "start_time": "2026-01-20T01:26:17.550483",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Setup & Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2365880a",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ce49ec5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T01:26:17.587605Z",
     "iopub.status.busy": "2026-01-20T01:26:17.587605Z",
     "iopub.status.idle": "2026-01-20T01:26:21.181519Z",
     "shell.execute_reply": "2026-01-20T01:26:21.181519Z"
    },
    "papermill": {
     "duration": 3.618583,
     "end_time": "2026-01-20T01:26:21.185946",
     "exception": true,
     "start_time": "2026-01-20T01:26:17.567363",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: C:\\Coding\\DataMining\n",
      "Source dir: C:\\Coding\\DataMining\\src\n",
      "Source dir exists: False\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSource dir exists: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSRC_DIR\u001b[38;5;241m.\u001b[39mexists()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Import custom modules\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# Training functions\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     train_logistic_regression,\n\u001b[0;32m     30\u001b[0m     train_decision_tree,\n\u001b[0;32m     31\u001b[0m     train_random_forest,\n\u001b[0;32m     32\u001b[0m     train_xgboost,\n\u001b[0;32m     33\u001b[0m     train_lightgbm,\n\u001b[0;32m     34\u001b[0m     train_all_models,\n\u001b[0;32m     35\u001b[0m     \n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m# Tuning\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     tune_hyperparameters,\n\u001b[0;32m     38\u001b[0m     cross_validate_model,\n\u001b[0;32m     39\u001b[0m     \n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# Prediction\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     predict,\n\u001b[0;32m     42\u001b[0m     predict_proba,\n\u001b[0;32m     43\u001b[0m     \n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m# Model I/O\u001b[39;00m\n\u001b[0;32m     45\u001b[0m     save_model,\n\u001b[0;32m     46\u001b[0m     load_model,\n\u001b[0;32m     47\u001b[0m     \n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m# Feature importance\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     get_feature_importance,\n\u001b[0;32m     50\u001b[0m     \n\u001b[0;32m     51\u001b[0m     \u001b[38;5;66;03m# Availability flags\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     XGBOOST_AVAILABLE,\n\u001b[0;32m     53\u001b[0m     LIGHTGBM_AVAILABLE\n\u001b[0;32m     54\u001b[0m )\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;66;03m# Metrics\u001b[39;00m\n\u001b[0;32m     58\u001b[0m     calculate_metrics,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     78\u001b[0m     evaluate_model\n\u001b[0;32m     79\u001b[0m )\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m‚úÖ Libraries imported successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'models'"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Configure\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# Add src to path - use absolute path for reliability\n",
    "NOTEBOOK_DIR = Path(os.path.abspath('')).resolve()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent\n",
    "SRC_DIR = PROJECT_ROOT / 'src'\n",
    "sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Source dir: {SRC_DIR}\")\n",
    "print(f\"Source dir exists: {SRC_DIR.exists()}\")\n",
    "\n",
    "# Import custom modules\n",
    "from models import (\n",
    "    # Training functions\n",
    "    train_logistic_regression,\n",
    "    train_decision_tree,\n",
    "    train_random_forest,\n",
    "    train_xgboost,\n",
    "    train_lightgbm,\n",
    "    train_all_models,\n",
    "    \n",
    "    # Tuning\n",
    "    tune_hyperparameters,\n",
    "    cross_validate_model,\n",
    "    \n",
    "    # Prediction\n",
    "    predict,\n",
    "    predict_proba,\n",
    "    \n",
    "    # Model I/O\n",
    "    save_model,\n",
    "    load_model,\n",
    "    \n",
    "    # Feature importance\n",
    "    get_feature_importance,\n",
    "    \n",
    "    # Availability flags\n",
    "    XGBOOST_AVAILABLE,\n",
    "    LIGHTGBM_AVAILABLE\n",
    ")\n",
    "\n",
    "from evaluation import (\n",
    "    # Metrics\n",
    "    calculate_metrics,\n",
    "    get_classification_report,\n",
    "    \n",
    "    # Plots\n",
    "    plot_confusion_matrix,\n",
    "    plot_roc_curve,\n",
    "    plot_roc_curves_comparison,\n",
    "    plot_pr_curve,\n",
    "    plot_pr_curves_comparison,\n",
    "    plot_feature_importance,\n",
    "    \n",
    "    # Model comparison\n",
    "    compare_models,\n",
    "    plot_model_comparison,\n",
    "    \n",
    "    # Threshold\n",
    "    find_optimal_threshold,\n",
    "    plot_threshold_analysis,\n",
    "    \n",
    "    # Pipeline\n",
    "    evaluate_model\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Libraries imported successfully!\")\n",
    "print(f\"   XGBoost available: {XGBOOST_AVAILABLE}\")\n",
    "print(f\"   LightGBM available: {LIGHTGBM_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a1c4aa",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 2. Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7f678d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_PROCESSED = PROJECT_ROOT / 'data' / 'processed'\n",
    "OUTPUT_DIR = PROJECT_ROOT / 'outputs'\n",
    "FIGURES_DIR = OUTPUT_DIR / 'figures'\n",
    "TABLES_DIR = OUTPUT_DIR / 'tables'\n",
    "MODELS_DIR = OUTPUT_DIR / 'models'\n",
    "\n",
    "# Create output directories if not exist\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TABLES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"üìÇ Loading processed data...\")\n",
    "\n",
    "# Load resampled training data (SMOTE applied)\n",
    "X_train = pd.read_csv(DATA_PROCESSED / 'X_train_resampled.csv')\n",
    "y_train = pd.read_csv(DATA_PROCESSED / 'y_train_resampled.csv').squeeze()\n",
    "\n",
    "# Load test data (original, not resampled)\n",
    "X_test = pd.read_csv(DATA_PROCESSED / 'X_test_encoded.csv')\n",
    "y_test = pd.read_csv(DATA_PROCESSED / 'y_test.csv').squeeze()\n",
    "\n",
    "print(f\"\\nüìä Training data (SMOTE resampled):\")\n",
    "print(f\"   X_train shape: {X_train.shape}\")\n",
    "print(f\"   y_train shape: {y_train.shape}\")\n",
    "print(f\"   Class distribution: {y_train.value_counts().to_dict()}\")\n",
    "\n",
    "print(f\"\\nüìä Test data:\")\n",
    "print(f\"   X_test shape: {X_test.shape}\")\n",
    "print(f\"   y_test shape: {y_test.shape}\")\n",
    "print(f\"   Class distribution: {y_test.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fab53bd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check column alignment between train and test\n",
    "train_cols = set(X_train.columns)\n",
    "test_cols = set(X_test.columns)\n",
    "\n",
    "if train_cols == test_cols:\n",
    "    print(\"‚úÖ Train and Test columns are aligned!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Column mismatch detected!\")\n",
    "    print(f\"   In train but not test: {train_cols - test_cols}\")\n",
    "    print(f\"   In test but not train: {test_cols - train_cols}\")\n",
    "    \n",
    "    # Align columns\n",
    "    common_cols = list(train_cols & test_cols)\n",
    "    X_train = X_train[common_cols]\n",
    "    X_test = X_test[common_cols]\n",
    "    print(f\"\\n‚úÖ Aligned to {len(common_cols)} common columns\")\n",
    "\n",
    "# Feature names for later use\n",
    "feature_names = list(X_train.columns)\n",
    "print(f\"\\nüìã Number of features: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323346a4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Quick look at data\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d89afae",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 3. Verify No Data Leakage\n",
    "\n",
    "‚ö†Ô∏è **CRITICAL**: Ensure we don't use columns that contain information about the outcome:\n",
    "- `reservation_status` (contains 'Canceled' directly)\n",
    "- `reservation_status_date`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace6db56",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for potential leakage columns\n",
    "leakage_cols = ['reservation_status', 'reservation_status_date', 'is_canceled']\n",
    "\n",
    "found_leakage = [col for col in leakage_cols if col in X_train.columns]\n",
    "\n",
    "if found_leakage:\n",
    "    print(f\"üö® LEAKAGE DETECTED! Found columns: {found_leakage}\")\n",
    "    print(\"   These columns should NOT be in training features!\")\n",
    "else:\n",
    "    print(\"‚úÖ No data leakage detected!\")\n",
    "    print(f\"   Checked for: {leakage_cols}\")\n",
    "    print(\"   None found in training features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f24d308",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "# PART A: BASELINE MODELS\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Logistic Regression (Baseline 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b89f1b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train Logistic Regression\n",
    "model_lr = train_logistic_regression(\n",
    "    X_train, y_train,\n",
    "    C=1.0,\n",
    "    class_weight='balanced',\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2905c39e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "y_pred_lr = model_lr.predict(X_test)\n",
    "y_proba_lr = model_lr.predict_proba(X_test)[:, 1]\n",
    "\n",
    "metrics_lr = calculate_metrics(y_test, y_pred_lr, y_proba_lr, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66845f97",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "plot_confusion_matrix(\n",
    "    y_test, y_pred_lr,\n",
    "    title='Confusion Matrix - Logistic Regression',\n",
    "    save_path=str(FIGURES_DIR / 'cm_logistic_regression.png'),\n",
    "    show=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9469db6c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 5. Decision Tree (Baseline 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f5d5ff",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train Decision Tree\n",
    "model_dt = train_decision_tree(\n",
    "    X_train, y_train,\n",
    "    max_depth=10,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    class_weight='balanced',\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c1f40a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "y_pred_dt = model_dt.predict(X_test)\n",
    "y_proba_dt = model_dt.predict_proba(X_test)[:, 1]\n",
    "\n",
    "metrics_dt = calculate_metrics(y_test, y_pred_dt, y_proba_dt, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455919ba",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "plot_confusion_matrix(\n",
    "    y_test, y_pred_dt,\n",
    "    title='Confusion Matrix - Decision Tree',\n",
    "    save_path=str(FIGURES_DIR / 'cm_decision_tree.png'),\n",
    "    show=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b31a01",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "# PART B: IMPROVED MODELS\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fff79c5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train Random Forest with default parameters first\n",
    "model_rf = train_random_forest(\n",
    "    X_train, y_train,\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    min_samples_split=10,\n",
    "    class_weight='balanced',\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9302d02c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "y_pred_rf = model_rf.predict(X_test)\n",
    "y_proba_rf = model_rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "metrics_rf = calculate_metrics(y_test, y_pred_rf, y_proba_rf, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2315172",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "plot_confusion_matrix(\n",
    "    y_test, y_pred_rf,\n",
    "    title='Confusion Matrix - Random Forest',\n",
    "    save_path=str(FIGURES_DIR / 'cm_random_forest.png'),\n",
    "    show=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1674b721",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 6.1 Random Forest Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c917fdd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for Random Forest\n",
    "# Using RandomizedSearch for faster results (can change to 'grid' for exhaustive search)\n",
    "\n",
    "# Custom parameter grid (smaller for faster execution)\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 15, 20],\n",
    "    'min_samples_split': [5, 10],\n",
    "    'min_samples_leaf': [2, 5],\n",
    "    'max_features': ['sqrt', 0.5]\n",
    "}\n",
    "\n",
    "best_rf, best_rf_params, best_rf_score = tune_hyperparameters(\n",
    "    'rf',\n",
    "    X_train, y_train,\n",
    "    param_grid=rf_param_grid,\n",
    "    search_method='random',\n",
    "    cv=3,  # Reduced CV for speed\n",
    "    scoring='f1',\n",
    "    n_iter=10,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c71d73",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate tuned Random Forest\n",
    "y_pred_rf_tuned = best_rf.predict(X_test)\n",
    "y_proba_rf_tuned = best_rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "metrics_rf_tuned = calculate_metrics(y_test, y_pred_rf_tuned, y_proba_rf_tuned, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b091be",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 7. XGBoost (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d597020",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if XGBOOST_AVAILABLE:\n",
    "    # Train XGBoost\n",
    "    model_xgb = train_xgboost(\n",
    "        X_train, y_train,\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred_xgb = model_xgb.predict(X_test)\n",
    "    y_proba_xgb = model_xgb.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    metrics_xgb = calculate_metrics(y_test, y_pred_xgb, y_proba_xgb, verbose=True)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è XGBoost not available. Install with: pip install xgboost\")\n",
    "    model_xgb = None\n",
    "    y_proba_xgb = None\n",
    "    metrics_xgb = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e90a483",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if XGBOOST_AVAILABLE and model_xgb is not None:\n",
    "    plot_confusion_matrix(\n",
    "        y_test, y_pred_xgb,\n",
    "        title='Confusion Matrix - XGBoost',\n",
    "        save_path=str(FIGURES_DIR / 'cm_xgboost.png'),\n",
    "        show=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0326559",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 8. LightGBM (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570d47ca",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if LIGHTGBM_AVAILABLE:\n",
    "    # Train LightGBM\n",
    "    model_lgb = train_lightgbm(\n",
    "        X_train, y_train,\n",
    "        n_estimators=100,\n",
    "        num_leaves=31,\n",
    "        learning_rate=0.1,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred_lgb = model_lgb.predict(X_test)\n",
    "    y_proba_lgb = model_lgb.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    metrics_lgb = calculate_metrics(y_test, y_pred_lgb, y_proba_lgb, verbose=True)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è LightGBM not available. Install with: pip install lightgbm\")\n",
    "    model_lgb = None\n",
    "    y_proba_lgb = None\n",
    "    metrics_lgb = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad6480c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if LIGHTGBM_AVAILABLE and model_lgb is not None:\n",
    "    plot_confusion_matrix(\n",
    "        y_test, y_pred_lgb,\n",
    "        title='Confusion Matrix - LightGBM',\n",
    "        save_path=str(FIGURES_DIR / 'cm_lightgbm.png'),\n",
    "        show=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877e0b37",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "# PART C: MODEL COMPARISON\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Compare All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115b6795",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Collect all results\n",
    "all_results = {\n",
    "    'Logistic Regression': metrics_lr,\n",
    "    'Decision Tree': metrics_dt,\n",
    "    'Random Forest': metrics_rf,\n",
    "    'Random Forest (Tuned)': metrics_rf_tuned\n",
    "}\n",
    "\n",
    "# Add XGBoost if available\n",
    "if metrics_xgb is not None:\n",
    "    all_results['XGBoost'] = metrics_xgb\n",
    "\n",
    "# Add LightGBM if available  \n",
    "if metrics_lgb is not None:\n",
    "    all_results['LightGBM'] = metrics_lgb\n",
    "\n",
    "# Create comparison table\n",
    "comparison_df = compare_models(\n",
    "    all_results,\n",
    "    metrics=['accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'pr_auc'],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e03265d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save comparison table\n",
    "comparison_df.to_csv(TABLES_DIR / 'model_comparison.csv')\n",
    "print(f\"‚úÖ Saved comparison table to {TABLES_DIR / 'model_comparison.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202f8df8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot model comparison\n",
    "plot_model_comparison(\n",
    "    comparison_df,\n",
    "    metrics=['accuracy', 'f1', 'roc_auc', 'pr_auc'],\n",
    "    title='Model Performance Comparison',\n",
    "    figsize=(14, 6),\n",
    "    save_path=str(FIGURES_DIR / 'model_comparison.png'),\n",
    "    show=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2105ee4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 10. ROC Curves Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02f1a15",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Collect predictions for ROC curves\n",
    "predictions_for_roc = {\n",
    "    'Logistic Regression': y_proba_lr,\n",
    "    'Decision Tree': y_proba_dt,\n",
    "    'Random Forest': y_proba_rf,\n",
    "    'Random Forest (Tuned)': y_proba_rf_tuned\n",
    "}\n",
    "\n",
    "if y_proba_xgb is not None:\n",
    "    predictions_for_roc['XGBoost'] = y_proba_xgb\n",
    "    \n",
    "if y_proba_lgb is not None:\n",
    "    predictions_for_roc['LightGBM'] = y_proba_lgb\n",
    "\n",
    "# Plot ROC curves\n",
    "plot_roc_curves_comparison(\n",
    "    y_test,\n",
    "    predictions_for_roc,\n",
    "    figsize=(10, 8),\n",
    "    save_path=str(FIGURES_DIR / 'roc_curves_comparison.png'),\n",
    "    show=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d170862",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 11. Precision-Recall Curves Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d432563",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot PR curves\n",
    "plot_pr_curves_comparison(\n",
    "    y_test,\n",
    "    predictions_for_roc,\n",
    "    figsize=(10, 8),\n",
    "    save_path=str(FIGURES_DIR / 'pr_curves_comparison.png'),\n",
    "    show=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e6c0a3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "# PART D: CROSS-VALIDATION\n",
    "\n",
    "---\n",
    "\n",
    "## 12. 5-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ff3059",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cross-validation for best model (Random Forest Tuned)\n",
    "print(\"üîÑ Running 5-fold Cross-Validation on Random Forest (Tuned)...\\n\")\n",
    "\n",
    "cv_results_rf = cross_validate_model(\n",
    "    best_rf,\n",
    "    X_train, y_train,\n",
    "    cv=5,\n",
    "    scoring=['accuracy', 'f1', 'precision', 'recall', 'roc_auc'],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47484f71",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cross-validation for Logistic Regression (baseline comparison)\n",
    "print(\"üîÑ Running 5-fold Cross-Validation on Logistic Regression...\\n\")\n",
    "\n",
    "cv_results_lr = cross_validate_model(\n",
    "    model_lr,\n",
    "    X_train, y_train,\n",
    "    cv=5,\n",
    "    scoring=['accuracy', 'f1', 'precision', 'recall', 'roc_auc'],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea28ac24",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create CV comparison table\n",
    "cv_comparison = pd.DataFrame({\n",
    "    'Metric': ['accuracy', 'f1', 'precision', 'recall', 'roc_auc'],\n",
    "    'Logistic (Mean)': [cv_results_lr[m].mean() for m in ['accuracy', 'f1', 'precision', 'recall', 'roc_auc']],\n",
    "    'Logistic (Std)': [cv_results_lr[m].std() for m in ['accuracy', 'f1', 'precision', 'recall', 'roc_auc']],\n",
    "    'RF Tuned (Mean)': [cv_results_rf[m].mean() for m in ['accuracy', 'f1', 'precision', 'recall', 'roc_auc']],\n",
    "    'RF Tuned (Std)': [cv_results_rf[m].std() for m in ['accuracy', 'f1', 'precision', 'recall', 'roc_auc']],\n",
    "})\n",
    "\n",
    "print(\"\\nüìä Cross-Validation Comparison:\")\n",
    "print(cv_comparison.round(4).to_string(index=False))\n",
    "\n",
    "# Save\n",
    "cv_comparison.to_csv(TABLES_DIR / 'cv_comparison.csv', index=False)\n",
    "print(f\"\\n‚úÖ Saved CV comparison to {TABLES_DIR / 'cv_comparison.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5ed984",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "# PART E: FEATURE IMPORTANCE ANALYSIS\n",
    "\n",
    "---\n",
    "\n",
    "## 13. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352b6810",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get feature importance from Random Forest\n",
    "importance_rf = get_feature_importance(\n",
    "    best_rf,\n",
    "    feature_names,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b40896f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "plot_feature_importance(\n",
    "    importance_rf,\n",
    "    top_n=20,\n",
    "    title='Feature Importance - Random Forest (Tuned)',\n",
    "    figsize=(10, 10),\n",
    "    color='forestgreen',\n",
    "    save_path=str(FIGURES_DIR / 'feature_importance_rf.png'),\n",
    "    show=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c117ac67",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Feature importance from Logistic Regression (coefficient magnitude)\n",
    "importance_lr = get_feature_importance(\n",
    "    model_lr,\n",
    "    feature_names,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e809bb82",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot LR feature importance\n",
    "plot_feature_importance(\n",
    "    importance_lr,\n",
    "    top_n=20,\n",
    "    title='Feature Importance - Logistic Regression (|coefficients|)',\n",
    "    figsize=(10, 10),\n",
    "    color='steelblue',\n",
    "    save_path=str(FIGURES_DIR / 'feature_importance_lr.png'),\n",
    "    show=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1c3589",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save feature importance to CSV\n",
    "importance_rf.to_csv(TABLES_DIR / 'feature_importance_rf.csv', index=False)\n",
    "importance_lr.to_csv(TABLES_DIR / 'feature_importance_lr.csv', index=False)\n",
    "print(f\"‚úÖ Saved feature importance tables to {TABLES_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf5930a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 14. Threshold Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6986d9c3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find optimal threshold for best model\n",
    "optimal_threshold, optimal_f1 = find_optimal_threshold(\n",
    "    y_test, y_proba_rf_tuned,\n",
    "    metric='f1',\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5569ebca",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot threshold analysis\n",
    "plot_threshold_analysis(\n",
    "    y_test, y_proba_rf_tuned,\n",
    "    figsize=(14, 5),\n",
    "    save_path=str(FIGURES_DIR / 'threshold_analysis.png'),\n",
    "    show=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b38ea2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compare default vs optimal threshold\n",
    "print(\"\\nüìä Comparison: Default (0.5) vs Optimal Threshold\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Default threshold\n",
    "y_pred_default = (y_proba_rf_tuned >= 0.5).astype(int)\n",
    "metrics_default = calculate_metrics(y_test, y_pred_default, y_proba_rf_tuned, verbose=False)\n",
    "\n",
    "# Optimal threshold\n",
    "y_pred_optimal = (y_proba_rf_tuned >= optimal_threshold).astype(int)\n",
    "metrics_optimal = calculate_metrics(y_test, y_pred_optimal, y_proba_rf_tuned, verbose=False)\n",
    "\n",
    "print(f\"\\nDefault Threshold (0.5):\")\n",
    "print(f\"   F1: {metrics_default['f1']:.4f}, Precision: {metrics_default['precision']:.4f}, Recall: {metrics_default['recall']:.4f}\")\n",
    "\n",
    "print(f\"\\nOptimal Threshold ({optimal_threshold:.2f}):\")\n",
    "print(f\"   F1: {metrics_optimal['f1']:.4f}, Precision: {metrics_optimal['precision']:.4f}, Recall: {metrics_optimal['recall']:.4f}\")\n",
    "\n",
    "print(f\"\\nImprovement in F1: {(metrics_optimal['f1'] - metrics_default['f1'])*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608420af",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "# PART F: SAVE MODELS\n",
    "\n",
    "---\n",
    "\n",
    "## 15. Save Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bb7572",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save all models\n",
    "save_model(model_lr, MODELS_DIR / 'logistic_regression.joblib', verbose=True)\n",
    "save_model(model_dt, MODELS_DIR / 'decision_tree.joblib', verbose=True)\n",
    "save_model(model_rf, MODELS_DIR / 'random_forest.joblib', verbose=True)\n",
    "save_model(best_rf, MODELS_DIR / 'random_forest_tuned.joblib', verbose=True)\n",
    "\n",
    "if model_xgb is not None:\n",
    "    save_model(model_xgb, MODELS_DIR / 'xgboost.joblib', verbose=True)\n",
    "    \n",
    "if model_lgb is not None:\n",
    "    save_model(model_lgb, MODELS_DIR / 'lightgbm.joblib', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b3f6f3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## 16. Summary & Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720bb8fc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìù SUMMARY OF CLASSIFICATION MODELS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"BASELINE MODELS\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"1. Logistic Regression:\")\n",
    "print(f\"   - F1 Score: {metrics_lr['f1']:.4f}\")\n",
    "print(f\"   - ROC-AUC: {metrics_lr.get('roc_auc', 'N/A'):.4f}\")\n",
    "print(f\"   - PR-AUC: {metrics_lr.get('pr_auc', 'N/A'):.4f}\")\n",
    "\n",
    "print(f\"\\n2. Decision Tree:\")\n",
    "print(f\"   - F1 Score: {metrics_dt['f1']:.4f}\")\n",
    "print(f\"   - ROC-AUC: {metrics_dt.get('roc_auc', 'N/A'):.4f}\")\n",
    "print(f\"   - PR-AUC: {metrics_dt.get('pr_auc', 'N/A'):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"IMPROVED MODELS\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"3. Random Forest (Tuned):\")\n",
    "print(f\"   - F1 Score: {metrics_rf_tuned['f1']:.4f}\")\n",
    "print(f\"   - ROC-AUC: {metrics_rf_tuned.get('roc_auc', 'N/A'):.4f}\")\n",
    "print(f\"   - PR-AUC: {metrics_rf_tuned.get('pr_auc', 'N/A'):.4f}\")\n",
    "print(f\"   - Best params: {best_rf_params}\")\n",
    "\n",
    "if metrics_xgb is not None:\n",
    "    print(f\"\\n4. XGBoost:\")\n",
    "    print(f\"   - F1 Score: {metrics_xgb['f1']:.4f}\")\n",
    "    print(f\"   - ROC-AUC: {metrics_xgb.get('roc_auc', 'N/A'):.4f}\")\n",
    "\n",
    "if metrics_lgb is not None:\n",
    "    print(f\"\\n5. LightGBM:\")\n",
    "    print(f\"   - F1 Score: {metrics_lgb['f1']:.4f}\")\n",
    "    print(f\"   - ROC-AUC: {metrics_lgb.get('roc_auc', 'N/A'):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Find best model\n",
    "best_model_name = comparison_df['f1'].idxmax()\n",
    "best_f1 = comparison_df.loc[best_model_name, 'f1']\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name}\")\n",
    "print(f\"   F1 Score: {best_f1:.4f}\")\n",
    "\n",
    "print(f\"\\nüìä Top 5 Important Features (from RF):\")\n",
    "for i, row in importance_rf.head(5).iterrows():\n",
    "    print(f\"   {i+1}. {row['feature']}: {row['importance_pct']:.2f}%\")\n",
    "\n",
    "print(f\"\\nüéØ Optimal Threshold: {optimal_threshold:.2f} (vs default 0.5)\")\n",
    "print(f\"   Improvement in F1: {(metrics_optimal['f1'] - metrics_default['f1'])*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"SAVED OUTPUTS\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"üìÅ Models: {MODELS_DIR}\")\n",
    "print(f\"üìÅ Figures: {FIGURES_DIR}\")\n",
    "print(f\"üìÅ Tables: {TABLES_DIR}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ CLASSIFICATION MODELING COMPLETE!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8.178966,
   "end_time": "2026-01-20T01:26:23.883262",
   "environment_variables": {},
   "exception": true,
   "input_path": "C:\\Coding\\DataMining\\Nhom12_BaiTapLon_DataMining\\notebooks\\04_modeling.ipynb",
   "output_path": "C:\\Coding\\DataMining\\Nhom12_BaiTapLon_DataMining\\outputs\\executed_notebooks\\04_modeling_20260120_082615.ipynb",
   "parameters": {},
   "start_time": "2026-01-20T01:26:15.704296",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}