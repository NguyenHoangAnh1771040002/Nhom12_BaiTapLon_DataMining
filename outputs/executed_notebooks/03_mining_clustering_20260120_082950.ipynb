{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b12e4ec",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [1]</a>'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adc6ef1",
   "metadata": {
    "papermill": {
     "duration": 0.009906,
     "end_time": "2026-01-20T01:29:52.589878",
     "exception": false,
     "start_time": "2026-01-20T01:29:52.579972",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Setup & Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2be340",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c32921c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T01:29:52.614405Z",
     "iopub.status.busy": "2026-01-20T01:29:52.614405Z",
     "iopub.status.idle": "2026-01-20T01:29:55.571784Z",
     "shell.execute_reply": "2026-01-20T01:29:55.571784Z"
    },
    "papermill": {
     "duration": 2.970795,
     "end_time": "2026-01-20T01:29:55.571784",
     "exception": true,
     "start_time": "2026-01-20T01:29:52.600989",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: C:\\Coding\\DataMining\n",
      "Source dir: C:\\Coding\\DataMining\\src\n",
      "Source dir exists: False\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mining'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSource dir exists: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSRC_DIR\u001b[38;5;241m.\u001b[39mexists()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Import custom modules\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmining\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# Association Rules\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     prepare_transactions,\n\u001b[0;32m     30\u001b[0m     run_apriori,\n\u001b[0;32m     31\u001b[0m     run_fpgrowth,\n\u001b[0;32m     32\u001b[0m     extract_rules,\n\u001b[0;32m     33\u001b[0m     filter_rules_by_consequent,\n\u001b[0;32m     34\u001b[0m     get_top_rules,\n\u001b[0;32m     35\u001b[0m     summarize_rules,\n\u001b[0;32m     36\u001b[0m     plot_rules_heatmap,\n\u001b[0;32m     37\u001b[0m     plot_support_confidence_scatter,\n\u001b[0;32m     38\u001b[0m     mine_association_rules,\n\u001b[0;32m     39\u001b[0m     MLXTEND_AVAILABLE,\n\u001b[0;32m     40\u001b[0m     \n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# Clustering\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     prepare_clustering_data,\n\u001b[0;32m     43\u001b[0m     find_optimal_k,\n\u001b[0;32m     44\u001b[0m     plot_elbow_silhouette,\n\u001b[0;32m     45\u001b[0m     run_kmeans,\n\u001b[0;32m     46\u001b[0m     run_dbscan,\n\u001b[0;32m     47\u001b[0m     run_hierarchical,\n\u001b[0;32m     48\u001b[0m     evaluate_clustering,\n\u001b[0;32m     49\u001b[0m     profile_clusters,\n\u001b[0;32m     50\u001b[0m     identify_high_risk_clusters,\n\u001b[0;32m     51\u001b[0m     plot_clusters_2d,\n\u001b[0;32m     52\u001b[0m     plot_cluster_profiles,\n\u001b[0;32m     53\u001b[0m     plot_cancellation_by_cluster,\n\u001b[0;32m     54\u001b[0m     cluster_bookings\n\u001b[0;32m     55\u001b[0m )\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Libraries imported successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   mlxtend available: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMLXTEND_AVAILABLE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mining'"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Configure\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# Add src to path - use absolute path for reliability\n",
    "NOTEBOOK_DIR = Path(os.path.abspath('')).resolve()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent\n",
    "SRC_DIR = PROJECT_ROOT / 'src'\n",
    "sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Source dir: {SRC_DIR}\")\n",
    "print(f\"Source dir exists: {SRC_DIR.exists()}\")\n",
    "\n",
    "# Import custom modules\n",
    "from mining import (\n",
    "    # Association Rules\n",
    "    prepare_transactions,\n",
    "    run_apriori,\n",
    "    run_fpgrowth,\n",
    "    extract_rules,\n",
    "    filter_rules_by_consequent,\n",
    "    get_top_rules,\n",
    "    summarize_rules,\n",
    "    plot_rules_heatmap,\n",
    "    plot_support_confidence_scatter,\n",
    "    mine_association_rules,\n",
    "    MLXTEND_AVAILABLE,\n",
    "    \n",
    "    # Clustering\n",
    "    prepare_clustering_data,\n",
    "    find_optimal_k,\n",
    "    plot_elbow_silhouette,\n",
    "    run_kmeans,\n",
    "    run_dbscan,\n",
    "    run_hierarchical,\n",
    "    evaluate_clustering,\n",
    "    profile_clusters,\n",
    "    identify_high_risk_clusters,\n",
    "    plot_clusters_2d,\n",
    "    plot_cluster_profiles,\n",
    "    plot_cancellation_by_cluster,\n",
    "    cluster_bookings\n",
    ")\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(f\"   mlxtend available: {MLXTEND_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c5bde3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99486eaa",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_RAW = Path.cwd().parent / 'data' / 'raw' / 'hotel_bookings.csv'\n",
    "DATA_PROCESSED = Path.cwd().parent / 'data' / 'processed'\n",
    "OUTPUT_DIR = Path.cwd().parent / 'outputs'\n",
    "FIGURES_DIR = OUTPUT_DIR / 'figures'\n",
    "TABLES_DIR = OUTPUT_DIR / 'tables'\n",
    "\n",
    "# Create output directories if not exist\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TABLES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load raw data for association rules (need original categorical values)\n",
    "df_raw = pd.read_csv(DATA_RAW)\n",
    "print(f\"Raw data shape: {df_raw.shape}\")\n",
    "\n",
    "# Load cleaned data if available\n",
    "train_path = DATA_PROCESSED / 'X_train.csv'\n",
    "if train_path.exists():\n",
    "    X_train = pd.read_csv(train_path)\n",
    "    y_train = pd.read_csv(DATA_PROCESSED / 'y_train.csv')['is_canceled']\n",
    "    print(f\"Processed train data shape: {X_train.shape}\")\n",
    "else:\n",
    "    print(\"âš ï¸ Processed data not found. Will use raw data.\")\n",
    "    X_train = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392ffec1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Quick look at data\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771e7e5a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "# PART A: ASSOCIATION RULES MINING\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Data Preparation for Association Rules\n",
    "\n",
    "Association rules yÃªu cáº§u dá»¯ liá»‡u dáº¡ng transaction (binary/categorical). Ta sáº½:\n",
    "1. Chá»n cÃ¡c cá»™t categorical quan trá»ng\n",
    "2. Bin cÃ¡c cá»™t numerical thÃ nh cÃ¡c nhÃ³m\n",
    "3. Chuyá»ƒn Ä‘á»•i sang dáº¡ng one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14af7969",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare data for association rules\n",
    "# Drop leakage columns first\n",
    "df_for_ar = df_raw.drop(columns=['reservation_status', 'reservation_status_date'], errors='ignore').copy()\n",
    "\n",
    "# Remove rows with missing agent (or fill with 'Unknown')\n",
    "df_for_ar = df_for_ar.dropna(subset=['country'])\n",
    "\n",
    "print(f\"Data shape after dropping leakage: {df_for_ar.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cae2a5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define columns for association rules\n",
    "categorical_cols = [\n",
    "    'hotel',\n",
    "    'is_canceled',\n",
    "    'meal',\n",
    "    'market_segment',\n",
    "    'distribution_channel',\n",
    "    'is_repeated_guest',\n",
    "    'deposit_type',\n",
    "    'customer_type'\n",
    "]\n",
    "\n",
    "# Columns to bin\n",
    "numerical_cols_to_bin = {\n",
    "    'lead_time': [0, 7, 30, 90, 180, float('inf')],\n",
    "    'adr': [0, 50, 100, 150, 200, float('inf')],\n",
    "    'total_of_special_requests': [0, 1, 2, float('inf')]\n",
    "}\n",
    "\n",
    "bin_labels = {\n",
    "    'lead_time': ['0-7d', '7-30d', '30-90d', '90-180d', '180d+'],\n",
    "    'adr': ['0-50', '50-100', '100-150', '150-200', '200+'],\n",
    "    'total_of_special_requests': ['0_req', '1_req', '2+_req']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344e73bd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create binned columns\n",
    "df_ar = df_for_ar[categorical_cols].copy()\n",
    "\n",
    "for col, bins in numerical_cols_to_bin.items():\n",
    "    df_ar[col] = pd.cut(\n",
    "        df_for_ar[col], \n",
    "        bins=bins, \n",
    "        labels=bin_labels[col],\n",
    "        include_lowest=True\n",
    "    )\n",
    "\n",
    "# Convert is_canceled to string for better readability\n",
    "df_ar['is_canceled'] = df_ar['is_canceled'].map({0: 'NotCanceled', 1: 'Canceled'})\n",
    "df_ar['is_repeated_guest'] = df_ar['is_repeated_guest'].map({0: 'NewGuest', 1: 'RepeatedGuest'})\n",
    "\n",
    "print(f\"Data for Association Rules: {df_ar.shape}\")\n",
    "df_ar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ad89c7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "df_ar.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0330a5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop rows with any missing values\n",
    "df_ar = df_ar.dropna()\n",
    "print(f\"Data shape after dropping NA: {df_ar.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2224234e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 4. Prepare Transaction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd00197d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare transactions using our module\n",
    "# The function will one-hot encode all categorical columns\n",
    "df_encoded = prepare_transactions(\n",
    "    df_ar,\n",
    "    columns=df_ar.columns.tolist(),\n",
    "    include_target=False,  # is_canceled already in df_ar\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897f83e7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# View encoded data\n",
    "print(f\"Encoded shape: {df_encoded.shape}\")\n",
    "print(f\"\\nAll column names:\")\n",
    "print(df_encoded.columns.tolist())\n",
    "\n",
    "# Find cancellation columns\n",
    "cancel_cols = [c for c in df_encoded.columns if 'cancel' in c.lower()]\n",
    "print(f\"\\nCancellation-related columns: {cancel_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae051f9d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 5. Run Association Rules Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d9d0e8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if MLXTEND_AVAILABLE:\n",
    "    # Run Apriori algorithm\n",
    "    frequent_itemsets = run_apriori(\n",
    "        df_encoded,\n",
    "        min_support=0.05,  # Items appearing in at least 5% of transactions\n",
    "        verbose=True\n",
    "    )\n",
    "else:\n",
    "    print(\"âš ï¸ mlxtend not available. Please install: pip install mlxtend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dc4058",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if MLXTEND_AVAILABLE and frequent_itemsets is not None:\n",
    "    # Extract association rules\n",
    "    rules = extract_rules(\n",
    "        frequent_itemsets,\n",
    "        metric='confidence',\n",
    "        min_threshold=0.5,  # Rules with confidence >= 50%\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    if rules is not None and len(rules) > 0:\n",
    "        print(f\"\\nðŸ“Š Total rules extracted: {len(rules)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f87253",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 6. Analyze Rules for Cancellation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1f00e6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find the exact column name for canceled\n",
    "cancel_col_name = [c for c in df_encoded.columns if 'Canceled' in c and 'Not' not in c]\n",
    "print(f\"Looking for column containing 'Canceled': {cancel_col_name}\")\n",
    "\n",
    "if MLXTEND_AVAILABLE and 'rules' in dir() and rules is not None and len(rules) > 0:\n",
    "    # Filter rules that predict cancellation\n",
    "    # Use the actual column name from the encoded data\n",
    "    if cancel_col_name:\n",
    "        cancel_rules = filter_rules_by_consequent(\n",
    "            rules,\n",
    "            consequent_items=cancel_col_name\n",
    "        )\n",
    "    else:\n",
    "        # Try different naming patterns\n",
    "        cancel_rules = filter_rules_by_consequent(\n",
    "            rules,\n",
    "            consequent_items=['is_canceled=Canceled', 'canceled=Canceled', 'is_canceled_Canceled']\n",
    "        )\n",
    "    \n",
    "    if cancel_rules is not None and len(cancel_rules) > 0:\n",
    "        print(f\"ðŸŽ¯ Rules predicting CANCELLATION: {len(cancel_rules)}\")\n",
    "    else:\n",
    "        print(\"âš ï¸ No cancellation rules found - trying to list available consequents:\")\n",
    "        # Show unique consequents to debug\n",
    "        unique_consequents = set()\n",
    "        for _, r in rules.iterrows():\n",
    "            for item in r['consequents']:\n",
    "                unique_consequents.add(item)\n",
    "        print(f\"Available consequents: {sorted(unique_consequents)[:20]}\")\n",
    "        cancel_rules = None\n",
    "else:\n",
    "    print(\"âš ï¸ No rules available to filter\")\n",
    "    cancel_rules = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d367554",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if MLXTEND_AVAILABLE and 'cancel_rules' in dir() and cancel_rules is not None and len(cancel_rules) > 0:\n",
    "    # Get top rules by lift (no verbose parameter)\n",
    "    top_cancel_rules = get_top_rules(\n",
    "        cancel_rules,\n",
    "        n=15,\n",
    "        sort_by='lift'\n",
    "    )\n",
    "    print(f\"ðŸ“Š Top 15 cancellation rules:\")\n",
    "    print(top_cancel_rules)\n",
    "else:\n",
    "    print(\"âš ï¸ No cancellation rules found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd55a2ce",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if MLXTEND_AVAILABLE and 'cancel_rules' in dir() and cancel_rules is not None and len(cancel_rules) > 0:\n",
    "    # Display top rules nicely\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ”¥ TOP 10 RULES PREDICTING CANCELLATION (by Lift)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for i, (_, rule) in enumerate(cancel_rules.nlargest(10, 'lift').iterrows(), 1):\n",
    "        antecedent = ', '.join(list(rule['antecedents']))\n",
    "        consequent = ', '.join(list(rule['consequents']))\n",
    "        \n",
    "        print(f\"\\nRule {i}:\")\n",
    "        print(f\"  IF: {antecedent}\")\n",
    "        print(f\"  THEN: {consequent}\")\n",
    "        print(f\"  Support: {rule['support']:.3f}, Confidence: {rule['confidence']:.3f}, Lift: {rule['lift']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f146edd1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 7. Rules for Non-Cancellation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe874d7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if MLXTEND_AVAILABLE and 'rules' in dir() and rules is not None and len(rules) > 0:\n",
    "    # Filter rules that predict NOT cancellation\n",
    "    not_cancel_rules = filter_rules_by_consequent(\n",
    "        rules,\n",
    "        consequent_items=['is_canceled=NotCanceled']\n",
    "    )\n",
    "    \n",
    "    if not_cancel_rules is not None and len(not_cancel_rules) > 0:\n",
    "        print(f\"âœ… Rules predicting NOT CANCELED: {len(not_cancel_rules)}\")\n",
    "        \n",
    "        # Top rules\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ðŸ”¥ TOP 10 RULES PREDICTING NOT CANCELED (by Lift)\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for i, (_, rule) in enumerate(not_cancel_rules.nlargest(10, 'lift').iterrows(), 1):\n",
    "            antecedent = ', '.join(list(rule['antecedents']))\n",
    "            consequent = ', '.join(list(rule['consequents']))\n",
    "            \n",
    "            print(f\"\\nRule {i}:\")\n",
    "            print(f\"  IF: {antecedent}\")\n",
    "            print(f\"  THEN: {consequent}\")\n",
    "            print(f\"  Support: {rule['support']:.3f}, Confidence: {rule['confidence']:.3f}, Lift: {rule['lift']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b399568a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 8. Visualize Association Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246628ce",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if MLXTEND_AVAILABLE and 'rules' in dir() and rules is not None and len(rules) > 0:\n",
    "    # Support-Confidence scatter plot\n",
    "    plot_support_confidence_scatter(\n",
    "        rules,\n",
    "        figsize=(12, 8),\n",
    "        save_path=str(FIGURES_DIR / 'association_rules_scatter.png')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2e7e38",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if MLXTEND_AVAILABLE and 'cancel_rules' in dir() and cancel_rules is not None and len(cancel_rules) >= 5:\n",
    "    # Heatmap for cancellation rules\n",
    "    plot_rules_heatmap(\n",
    "        cancel_rules.head(15),\n",
    "        figsize=(14, 10),\n",
    "        save_path=str(FIGURES_DIR / 'cancellation_rules_heatmap.png')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882ad42d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 9. Summary of Association Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c489d9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if MLXTEND_AVAILABLE and 'rules' in dir() and rules is not None and len(rules) > 0:\n",
    "    # Summary statistics (no verbose parameter)\n",
    "    summary = summarize_rules(rules, name=\"All Rules\")\n",
    "    print(\"ðŸ“Š Association Rules Summary:\")\n",
    "    print(summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f066fd9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save rules to file\n",
    "if MLXTEND_AVAILABLE and 'cancel_rules' in dir() and cancel_rules is not None and len(cancel_rules) > 0:\n",
    "    # Convert frozensets to strings for saving\n",
    "    rules_to_save = cancel_rules.copy()\n",
    "    rules_to_save['antecedents'] = rules_to_save['antecedents'].apply(lambda x: ', '.join(list(x)))\n",
    "    rules_to_save['consequents'] = rules_to_save['consequents'].apply(lambda x: ', '.join(list(x)))\n",
    "    \n",
    "    rules_to_save.to_csv(TABLES_DIR / 'association_rules_cancellation.csv', index=False)\n",
    "    print(f\"âœ… Saved cancellation rules to {TABLES_DIR / 'association_rules_cancellation.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fa3fd7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "# PART B: CUSTOMER CLUSTERING\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Prepare Data for Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8af2e8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use raw data for clustering (to have access to target for profiling)\n",
    "# Clean the data first\n",
    "df_cluster = df_raw.drop(columns=['reservation_status', 'reservation_status_date'], errors='ignore').copy()\n",
    "\n",
    "# Handle missing values for clustering features\n",
    "df_cluster['children'] = df_cluster['children'].fillna(0)\n",
    "df_cluster['agent'] = df_cluster['agent'].fillna(0)\n",
    "df_cluster['company'] = df_cluster['company'].fillna(0)\n",
    "\n",
    "# Create derived features if not exist\n",
    "if 'total_nights' not in df_cluster.columns:\n",
    "    df_cluster['total_nights'] = df_cluster['stays_in_weekend_nights'] + df_cluster['stays_in_week_nights']\n",
    "    \n",
    "if 'total_guests' not in df_cluster.columns:\n",
    "    df_cluster['total_guests'] = df_cluster['adults'] + df_cluster['children'] + df_cluster['babies']\n",
    "\n",
    "print(f\"Data for clustering: {df_cluster.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2667e56b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define clustering features\n",
    "clustering_features = [\n",
    "    'lead_time',\n",
    "    'total_nights',\n",
    "    'total_guests',\n",
    "    'adr',\n",
    "    'total_of_special_requests',\n",
    "    'booking_changes',\n",
    "    'previous_cancellations',\n",
    "    'previous_bookings_not_canceled',\n",
    "    'days_in_waiting_list',\n",
    "    'is_repeated_guest'\n",
    "]\n",
    "\n",
    "# Verify features exist\n",
    "available_features = [f for f in clustering_features if f in df_cluster.columns]\n",
    "print(f\"Available clustering features: {available_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1175cd70",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare clustering data using our module\n",
    "X_scaled, scaler, feature_names = prepare_clustering_data(\n",
    "    df_cluster,\n",
    "    features=available_features,\n",
    "    scaling_method='standard',\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d767037e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 11. Find Optimal Number of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135425be",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sample data for faster computation (clustering can be slow on large data)\n",
    "SAMPLE_SIZE = 20000\n",
    "\n",
    "if len(X_scaled) > SAMPLE_SIZE:\n",
    "    np.random.seed(42)\n",
    "    sample_idx = np.random.choice(len(X_scaled), SAMPLE_SIZE, replace=False)\n",
    "    X_sample = X_scaled.iloc[sample_idx]\n",
    "    df_sample = df_cluster.iloc[sample_idx]\n",
    "    print(f\"Using sample of {SAMPLE_SIZE} rows for faster computation\")\n",
    "else:\n",
    "    X_sample = X_scaled\n",
    "    df_sample = df_cluster\n",
    "    print(f\"Using full data: {len(X_sample)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99afa483",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find optimal k\n",
    "optimal_k_results = find_optimal_k(\n",
    "    X_sample,\n",
    "    k_range=(2, 11),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a64fbc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot elbow and silhouette\n",
    "plot_elbow_silhouette(\n",
    "    optimal_k_results,\n",
    "    figsize=(14, 5),\n",
    "    save_path=str(FIGURES_DIR / 'clustering_optimal_k.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac02bb4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 12. Run KMeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58697e67",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use optimal k from silhouette method (or choose manually)\n",
    "optimal_k = optimal_k_results['optimal_k_silhouette']\n",
    "print(f\"Using k = {optimal_k}\")\n",
    "\n",
    "# Run KMeans\n",
    "labels_kmeans, kmeans_model = run_kmeans(\n",
    "    X_sample,\n",
    "    n_clusters=optimal_k,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb834b3f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate clustering\n",
    "metrics_kmeans = evaluate_clustering(X_sample, labels_kmeans, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec200e9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize clusters with PCA\n",
    "plot_clusters_2d(\n",
    "    X_sample,\n",
    "    labels_kmeans,\n",
    "    method='pca',\n",
    "    title=f'KMeans Clustering (k={optimal_k}) - PCA Visualization',\n",
    "    figsize=(10, 8),\n",
    "    save_path=str(FIGURES_DIR / 'kmeans_clusters_pca.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933fb2fe",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 13. Profile Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fe23f6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Profile clusters\n",
    "profiles_kmeans = profile_clusters(\n",
    "    df_sample,\n",
    "    labels_kmeans,\n",
    "    features=feature_names,\n",
    "    target_col='is_canceled',\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017ece54",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Identify high-risk clusters\n",
    "high_risk_clusters = identify_high_risk_clusters(\n",
    "    profiles_kmeans,\n",
    "    threshold_pct=50.0,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b0d80f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot cancellation rate by cluster\n",
    "plot_cancellation_by_cluster(\n",
    "    profiles_kmeans,\n",
    "    figsize=(10, 6),\n",
    "    save_path=str(FIGURES_DIR / 'kmeans_cancellation_by_cluster.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b5b4f1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot cluster profiles heatmap\n",
    "plot_cluster_profiles(\n",
    "    profiles_kmeans,\n",
    "    features=feature_names,\n",
    "    figsize=(14, 8),\n",
    "    save_path=str(FIGURES_DIR / 'kmeans_cluster_profiles.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c48658",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 14. Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3addd92f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run Hierarchical clustering with same k\n",
    "labels_hc, hc_model = run_hierarchical(\n",
    "    X_sample,\n",
    "    n_clusters=optimal_k,\n",
    "    linkage='ward',\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adaf74c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate hierarchical clustering\n",
    "metrics_hc = evaluate_clustering(X_sample, labels_hc, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e727284",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize hierarchical clusters\n",
    "plot_clusters_2d(\n",
    "    X_sample,\n",
    "    labels_hc,\n",
    "    method='pca',\n",
    "    title=f'Hierarchical Clustering (k={optimal_k}) - PCA Visualization',\n",
    "    figsize=(10, 8),\n",
    "    save_path=str(FIGURES_DIR / 'hierarchical_clusters_pca.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781f220f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 15. DBSCAN Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8eec3e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DBSCAN - density-based clustering\n",
    "# Note: DBSCAN doesn't require k, but needs eps and min_samples\n",
    "labels_dbscan, dbscan_model = run_dbscan(\n",
    "    X_sample,\n",
    "    eps=1.5,  # May need tuning\n",
    "    min_samples=50,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ec2173",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate DBSCAN (if more than 1 cluster)\n",
    "n_clusters_dbscan = len(set(labels_dbscan)) - (1 if -1 in labels_dbscan else 0)\n",
    "if n_clusters_dbscan >= 2:\n",
    "    metrics_dbscan = evaluate_clustering(X_sample, labels_dbscan, verbose=True)\n",
    "else:\n",
    "    print(\"âš ï¸ DBSCAN found less than 2 clusters, try different parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ede8af",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 16. Compare Clustering Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82058c7b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compare clustering methods\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Method': ['KMeans', 'Hierarchical', 'DBSCAN'],\n",
    "    'n_clusters': [\n",
    "        metrics_kmeans.get('n_clusters', 'N/A'),\n",
    "        metrics_hc.get('n_clusters', 'N/A'),\n",
    "        n_clusters_dbscan if n_clusters_dbscan >= 2 else 'N/A'\n",
    "    ],\n",
    "    'Silhouette': [\n",
    "        metrics_kmeans.get('silhouette_score', 'N/A'),\n",
    "        metrics_hc.get('silhouette_score', 'N/A'),\n",
    "        metrics_dbscan.get('silhouette_score', 'N/A') if n_clusters_dbscan >= 2 else 'N/A'\n",
    "    ],\n",
    "    'Davies-Bouldin': [\n",
    "        metrics_kmeans.get('davies_bouldin_score', 'N/A'),\n",
    "        metrics_hc.get('davies_bouldin_score', 'N/A'),\n",
    "        metrics_dbscan.get('davies_bouldin_score', 'N/A') if n_clusters_dbscan >= 2 else 'N/A'\n",
    "    ],\n",
    "    'Calinski-Harabasz': [\n",
    "        metrics_kmeans.get('calinski_harabasz_score', 'N/A'),\n",
    "        metrics_hc.get('calinski_harabasz_score', 'N/A'),\n",
    "        metrics_dbscan.get('calinski_harabasz_score', 'N/A') if n_clusters_dbscan >= 2 else 'N/A'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“Š CLUSTERING METHODS COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"\\nâœ… Best Silhouette: Higher is better (range: -1 to 1)\")\n",
    "print(\"âœ… Best Davies-Bouldin: Lower is better\")\n",
    "print(\"âœ… Best Calinski-Harabasz: Higher is better\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7937f36b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 17. Detailed Cluster Analysis (Best Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d89144",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use KMeans as the primary method (usually performs well)\n",
    "# Detailed analysis of each cluster\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ” DETAILED CLUSTER ANALYSIS (KMEANS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Add cluster labels to sample data\n",
    "df_sample_clustered = df_sample.copy()\n",
    "df_sample_clustered['cluster'] = labels_kmeans\n",
    "\n",
    "for cluster in sorted(df_sample_clustered['cluster'].unique()):\n",
    "    cluster_data = df_sample_clustered[df_sample_clustered['cluster'] == cluster]\n",
    "    \n",
    "    print(f\"\\n\\n{'='*60}\")\n",
    "    print(f\"ðŸ“Š CLUSTER {cluster}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Size\n",
    "    size = len(cluster_data)\n",
    "    pct = size / len(df_sample_clustered) * 100\n",
    "    print(f\"\\nðŸ“ˆ Size: {size:,} bookings ({pct:.1f}%)\")\n",
    "    \n",
    "    # Cancellation rate\n",
    "    cancel_rate = cluster_data['is_canceled'].mean() * 100\n",
    "    print(f\"âŒ Cancellation Rate: {cancel_rate:.1f}%\")\n",
    "    \n",
    "    # Key characteristics\n",
    "    print(f\"\\nðŸ“‹ Key Characteristics:\")\n",
    "    for feat in feature_names:\n",
    "        if feat in cluster_data.columns:\n",
    "            mean_val = cluster_data[feat].mean()\n",
    "            median_val = cluster_data[feat].median()\n",
    "            overall_mean = df_sample_clustered[feat].mean()\n",
    "            diff = ((mean_val - overall_mean) / overall_mean * 100) if overall_mean != 0 else 0\n",
    "            \n",
    "            direction = \"â†‘\" if diff > 10 else \"â†“\" if diff < -10 else \"â†’\"\n",
    "            print(f\"   {feat}: mean={mean_val:.2f} ({direction} {abs(diff):.0f}% vs overall)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3752b40",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 18. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dcfcf9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save cluster profiles\n",
    "profiles_kmeans.to_csv(TABLES_DIR / 'cluster_profiles_kmeans.csv', index=False)\n",
    "print(f\"âœ… Saved cluster profiles to {TABLES_DIR / 'cluster_profiles_kmeans.csv'}\")\n",
    "\n",
    "# Save comparison\n",
    "comparison_df.to_csv(TABLES_DIR / 'clustering_comparison.csv', index=False)\n",
    "print(f\"âœ… Saved clustering comparison to {TABLES_DIR / 'clustering_comparison.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32b6472",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## 19. Summary & Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fdcda1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“ SUMMARY OF DATA MINING ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"PART A: ASSOCIATION RULES\")\n",
    "print(\"-\"*40)\n",
    "if MLXTEND_AVAILABLE and 'rules' in dir() and rules is not None:\n",
    "    print(f\"âœ… Total rules found: {len(rules)}\")\n",
    "    if 'cancel_rules' in dir() and cancel_rules is not None:\n",
    "        print(f\"âœ… Rules predicting cancellation: {len(cancel_rules)}\")\n",
    "        print(f\"\\nðŸ”‘ Key factors leading to cancellation:\")\n",
    "        # Extract top antecedents\n",
    "        if len(cancel_rules) > 0:\n",
    "            top_rules = cancel_rules.nlargest(5, 'lift')\n",
    "            for _, rule in top_rules.iterrows():\n",
    "                print(f\"   - {', '.join(list(rule['antecedents']))}\")\n",
    "else:\n",
    "    print(\"âš ï¸ Association rules not computed (mlxtend not available)\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"PART B: CUSTOMER CLUSTERING\")\n",
    "print(\"-\"*40)\n",
    "print(f\"âœ… Optimal number of clusters: {optimal_k}\")\n",
    "print(f\"âœ… Best silhouette score (KMeans): {metrics_kmeans['silhouette_score']:.4f}\")\n",
    "print(f\"\\nðŸš¨ High-risk clusters (cancel rate > 50%): {high_risk_clusters}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"BUSINESS INSIGHTS\")\n",
    "print(\"-\"*40)\n",
    "print(\"\\n1. Association Rules can identify booking patterns that lead to cancellation\")\n",
    "print(\"2. Customer segmentation reveals distinct groups with varying cancellation risks\")\n",
    "print(\"3. High-risk clusters can be targeted with retention strategies\")\n",
    "print(\"4. These insights can be used to:\")\n",
    "print(\"   - Implement dynamic pricing for high-risk segments\")\n",
    "print(\"   - Offer incentives to reduce cancellations\")\n",
    "print(\"   - Improve overbooking strategies\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… MINING & CLUSTERING ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5.297796,
   "end_time": "2026-01-20T01:29:56.047137",
   "environment_variables": {},
   "exception": true,
   "input_path": "C:\\Coding\\DataMining\\Nhom12_BaiTapLon_DataMining\\notebooks\\03_mining_clustering.ipynb",
   "output_path": "C:\\Coding\\DataMining\\Nhom12_BaiTapLon_DataMining\\outputs\\executed_notebooks\\03_mining_clustering_20260120_082950.ipynb",
   "parameters": {},
   "start_time": "2026-01-20T01:29:50.749341",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}