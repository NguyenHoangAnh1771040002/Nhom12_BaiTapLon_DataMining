{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1d33f3a",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [2]</a>'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56244bd",
   "metadata": {
    "papermill": {
     "duration": 0.01208,
     "end_time": "2026-01-20T01:29:46.378387",
     "exception": false,
     "start_time": "2026-01-20T01:29:46.366307",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Setup & Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf47313b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T01:29:46.397959Z",
     "iopub.status.busy": "2026-01-20T01:29:46.397959Z",
     "iopub.status.idle": "2026-01-20T01:29:49.374901Z",
     "shell.execute_reply": "2026-01-20T01:29:49.374901Z"
    },
    "papermill": {
     "duration": 2.98672,
     "end_time": "2026-01-20T01:29:49.374901",
     "exception": false,
     "start_time": "2026-01-20T01:29:46.388181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully!\n",
      "üìÅ Project root: C:\\Coding\\DataMining\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import custom modules\n",
    "from src.data.loader import load_raw_data, get_data_info\n",
    "from src.data.cleaner import (\n",
    "    clean_data, \n",
    "    drop_leakage_columns,\n",
    "    handle_missing_values,\n",
    "    handle_outliers,\n",
    "    handle_adr_outliers,\n",
    "    get_missing_summary,\n",
    "    save_artifacts,\n",
    "    encode_categorical,\n",
    "    scale_numerical\n",
    ")\n",
    "from src.features.builder import (\n",
    "    create_all_features,\n",
    "    prepare_for_association_rules,\n",
    "    get_feature_list\n",
    ")\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Plot settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"üìÅ Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ac1b9b",
   "metadata": {
    "papermill": {
     "duration": 0.007027,
     "end_time": "2026-01-20T01:29:49.391002",
     "exception": false,
     "start_time": "2026-01-20T01:29:49.383975",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Load Configuration & Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baa8579",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d05df9ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T01:29:49.408058Z",
     "iopub.status.busy": "2026-01-20T01:29:49.406597Z",
     "iopub.status.idle": "2026-01-20T01:29:50.131286Z",
     "shell.execute_reply": "2026-01-20T01:29:50.129873Z"
    },
    "papermill": {
     "duration": 0.734263,
     "end_time": "2026-01-20T01:29:50.131286",
     "exception": true,
     "start_time": "2026-01-20T01:29:49.397023",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Coding\\\\DataMining\\\\configs\\\\params.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load configuration\u001b[39;00m\n\u001b[0;32m      2\u001b[0m config_path \u001b[38;5;241m=\u001b[39m project_root \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfigs\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      4\u001b[0m     config \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39msafe_load(f)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Extract parameters\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\lab\\lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Coding\\\\DataMining\\\\configs\\\\params.yaml'"
     ]
    }
   ],
   "source": [
    "# Load configuration\n",
    "config_path = project_root / 'configs' / 'params.yaml'\n",
    "with open(config_path, 'r', encoding='utf-8') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Extract parameters\n",
    "SEED = config['seed']\n",
    "TEST_SIZE = config['split']['test_size']\n",
    "TARGET = config['target']\n",
    "\n",
    "print(f\"‚öôÔ∏è Configuration loaded:\")\n",
    "print(f\"   - Random Seed: {SEED}\")\n",
    "print(f\"   - Test Size: {TEST_SIZE}\")\n",
    "print(f\"   - Target Column: {TARGET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaaaa85",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "data_path = project_root / 'data' / 'raw' / 'hotel_bookings.csv'\n",
    "df_raw = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"üìä Raw Data Loaded:\")\n",
    "print(f\"   Shape: {df_raw.shape}\")\n",
    "print(f\"   Memory Usage: {df_raw.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec4cbd8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 3. Th·ªëng K√™ Tr∆∞·ªõc Ti·ªÅn X·ª≠ L√Ω\n",
    "\n",
    "Tr∆∞·ªõc khi x·ª≠ l√Ω, h√£y xem t√¨nh tr·∫°ng d·ªØ li·ªáu hi·ªán t·∫°i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c207257",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Overview before cleaning\n",
    "print(\"=\"*70)\n",
    "print(\"üìã TH·ªêNG K√ä TR∆Ø·ªöC TI·ªÄN X·ª¨ L√ù\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüìä Shape: {df_raw.shape[0]:,} rows √ó {df_raw.shape[1]} columns\")\n",
    "\n",
    "# Target distribution\n",
    "target_dist = df_raw[TARGET].value_counts()\n",
    "target_pct = df_raw[TARGET].value_counts(normalize=True) * 100\n",
    "print(f\"\\nüéØ Target Distribution ({TARGET}):\")\n",
    "print(f\"   - Not Canceled (0): {target_dist[0]:,} ({target_pct[0]:.2f}%)\")\n",
    "print(f\"   - Canceled (1): {target_dist[1]:,} ({target_pct[1]:.2f}%)\")\n",
    "print(f\"   ‚Üí Imbalance Ratio: {target_dist[0]/target_dist[1]:.2f}:1\")\n",
    "\n",
    "# Missing values\n",
    "missing_total = df_raw.isnull().sum().sum()\n",
    "print(f\"\\n‚ùå Missing Values Total: {missing_total:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c350f154",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Detailed missing values\n",
    "missing_summary = get_missing_summary(df_raw)\n",
    "if len(missing_summary) > 0:\n",
    "    print(\"\\nüìã Chi ti·∫øt Missing Values:\")\n",
    "    display(missing_summary.style.format({'Missing_Pct': '{:.2f}%'}))\n",
    "else:\n",
    "    print(\"\\n‚úÖ Kh√¥ng c√≥ missing values!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4026ad8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 4. Ti·ªÅn X·ª≠ L√Ω D·ªØ Li·ªáu\n",
    "\n",
    "### 4.1. Lo·∫°i b·ªè Data Leakage Columns\n",
    "\n",
    "‚ö†Ô∏è **QUAN TR·ªåNG:** C√°c c·ªôt `reservation_status` v√† `reservation_status_date` ch·ª©a th√¥ng tin v·ªÅ k·∫øt qu·∫£ ƒë·∫∑t ph√≤ng (Check-Out/Canceled/No-Show) - ƒë√¢y l√† th√¥ng tin **sau khi** booking ƒë√£ k·∫øt th√∫c, g√¢y ra **data leakage**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d52f577",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Drop leakage columns\n",
    "print(\"üîí B∆Ø·ªöC 1: Lo·∫°i b·ªè Data Leakage Columns\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Check leakage columns before dropping\n",
    "leakage_cols = ['reservation_status', 'reservation_status_date']\n",
    "print(\"\\n‚ö†Ô∏è C√°c c·ªôt g√¢y Data Leakage:\")\n",
    "for col in leakage_cols:\n",
    "    if col in df_raw.columns:\n",
    "        print(f\"   - {col}: {df_raw[col].nunique()} unique values\")\n",
    "        if col == 'reservation_status':\n",
    "            print(f\"     Values: {df_raw[col].value_counts().to_dict()}\")\n",
    "\n",
    "# Drop leakage columns\n",
    "df_step1 = drop_leakage_columns(df_raw, leakage_cols, verbose=True)\n",
    "print(f\"\\n‚úÖ Shape sau khi drop: {df_step1.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4351d0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 4.2. X·ª≠ L√Ω Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc36040",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 2: Handle missing values\n",
    "print(\"üîß B∆Ø·ªöC 2: X·ª≠ l√Ω Missing Values\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Use automatic strategy (defined in cleaner.py)\n",
    "df_step2 = handle_missing_values(df_step1, strategy='auto', verbose=True)\n",
    "\n",
    "# Verify no missing values remain\n",
    "remaining_missing = df_step2.isnull().sum().sum()\n",
    "print(f\"\\n‚úÖ Missing values c√≤n l·∫°i: {remaining_missing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e1860c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 4.3. X·ª≠ L√Ω Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c750205a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 3: Handle outliers\n",
    "print(\"üìà B∆Ø·ªöC 3: X·ª≠ l√Ω Outliers\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Special handling for ADR (Average Daily Rate)\n",
    "print(\"\\nüè∑Ô∏è ADR Statistics Before:\")\n",
    "print(f\"   Min: {df_step2['adr'].min():.2f}\")\n",
    "print(f\"   Max: {df_step2['adr'].max():.2f}\")\n",
    "print(f\"   Mean: {df_step2['adr'].mean():.2f}\")\n",
    "print(f\"   Negative values: {(df_step2['adr'] < 0).sum()}\")\n",
    "\n",
    "# Handle ADR outliers\n",
    "df_step3a = handle_adr_outliers(df_step2, min_adr=0, max_adr=5000, verbose=True)\n",
    "\n",
    "# Handle other outliers using IQR method with capping\n",
    "df_step3 = handle_outliers(\n",
    "    df_step3a,\n",
    "    method='iqr',\n",
    "    threshold=1.5,\n",
    "    strategy='cap',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Shape sau khi x·ª≠ l√Ω outliers: {df_step3.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1930693e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 5. Feature Engineering\n",
    "\n",
    "T·∫°o c√°c features m·ªõi t·ª´ d·ªØ li·ªáu ƒë√£ l√†m s·∫°ch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c1ee67",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply all feature engineering\n",
    "df_features = create_all_features(df_step3, config_path=str(config_path), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb720b63",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List all new features\n",
    "feature_list = get_feature_list()\n",
    "\n",
    "print(\"\\nüìã DANH S√ÅCH FEATURES M·ªöI:\")\n",
    "print(\"=\"*50)\n",
    "for category, features in feature_list.items():\n",
    "    print(f\"\\nüîπ {category.upper()}:\")\n",
    "    for feat in features:\n",
    "        if feat in df_features.columns:\n",
    "            print(f\"   ‚úì {feat}\")\n",
    "        else:\n",
    "            print(f\"   ‚úó {feat} (not created)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2524cbc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Quick statistics of new features\n",
    "new_features = [\n",
    "    'total_guests', 'total_nights', 'is_family',\n",
    "    'is_summer', 'is_peak_season', \n",
    "    'has_canceled_before', 'is_returning_customer',\n",
    "    'room_type_changed', 'deposit_required'\n",
    "]\n",
    "\n",
    "print(\"\\nüìä TH·ªêNG K√ä FEATURES M·ªöI:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for feat in new_features:\n",
    "    if feat in df_features.columns:\n",
    "        if df_features[feat].dtype in ['int64', 'float64']:\n",
    "            if df_features[feat].nunique() <= 2:  # Binary feature\n",
    "                pct = df_features[feat].mean() * 100\n",
    "                print(f\"{feat:30s}: {pct:6.2f}% = 1\")\n",
    "            else:\n",
    "                mean = df_features[feat].mean()\n",
    "                print(f\"{feat:30s}: mean = {mean:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8ea08a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 6. Th·ªëng K√™ Sau Ti·ªÅn X·ª≠ L√Ω"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662e84d8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Comparison: Before vs After\n",
    "print(\"=\"*70)\n",
    "print(\"üìä SO S√ÅNH TR∆Ø·ªöC - SAU TI·ªÄN X·ª¨ L√ù\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Metric': ['Rows', 'Columns', 'Missing Values', 'Memory (MB)'],\n",
    "    'Before': [\n",
    "        f\"{df_raw.shape[0]:,}\",\n",
    "        df_raw.shape[1],\n",
    "        f\"{df_raw.isnull().sum().sum():,}\",\n",
    "        f\"{df_raw.memory_usage(deep=True).sum() / 1024**2:.2f}\"\n",
    "    ],\n",
    "    'After': [\n",
    "        f\"{df_features.shape[0]:,}\",\n",
    "        df_features.shape[1],\n",
    "        f\"{df_features.isnull().sum().sum():,}\",\n",
    "        f\"{df_features.memory_usage(deep=True).sum() / 1024**2:.2f}\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "display(comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e05c32b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Columns removed and added\n",
    "removed_cols = set(df_raw.columns) - set(df_features.columns)\n",
    "added_cols = set(df_features.columns) - set(df_raw.columns)\n",
    "\n",
    "print(f\"\\nüóëÔ∏è Columns REMOVED ({len(removed_cols)}):\")\n",
    "for col in sorted(removed_cols):\n",
    "    print(f\"   - {col}\")\n",
    "\n",
    "print(f\"\\n‚ûï Columns ADDED ({len(added_cols)}):\")\n",
    "for col in sorted(added_cols):\n",
    "    print(f\"   + {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ccb8cb",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 7. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a000ac14",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df_features.drop(columns=[TARGET])\n",
    "y = df_features[TARGET]\n",
    "\n",
    "print(f\"üìä Features shape: {X.shape}\")\n",
    "print(f\"üéØ Target shape: {y.shape}\")\n",
    "print(f\"\\nüéØ Target distribution:\")\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8a3221",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train/Test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=SEED,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"üìä TRAIN/TEST SPLIT (Stratified)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nüîπ Training Set:\")\n",
    "print(f\"   X_train: {X_train.shape}\")\n",
    "print(f\"   y_train: {y_train.shape}\")\n",
    "print(f\"   Canceled ratio: {y_train.mean()*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nüîπ Test Set:\")\n",
    "print(f\"   X_test: {X_test.shape}\")\n",
    "print(f\"   y_test: {y_test.shape}\")\n",
    "print(f\"   Canceled ratio: {y_test.mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3639c921",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 8. X·ª≠ L√Ω Imbalance (SMOTE)\n",
    "\n",
    "Dataset c√≥ t·ª∑ l·ªá hu·ª∑ ~37%, kh√¥ng qu√° nghi√™m tr·ªçng nh∆∞ng v·∫´n n√™n x·ª≠ l√Ω."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5051d080",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Try to import SMOTE\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    SMOTE_AVAILABLE = True\n",
    "    print(\"‚úÖ SMOTE is available\")\n",
    "except ImportError:\n",
    "    SMOTE_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è imbalanced-learn not installed. Run: pip install imbalanced-learn\")\n",
    "    print(\"   Skipping SMOTE. Will use class_weight instead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f18fc03",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select only numerical columns for SMOTE\n",
    "# SMOTE requires numerical data\n",
    "numerical_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"üìä Numerical columns: {len(numerical_cols)}\")\n",
    "print(f\"üìä Categorical columns: {len(categorical_cols)}\")\n",
    "\n",
    "if categorical_cols:\n",
    "    print(f\"\\n‚ö†Ô∏è Categorical columns found: {categorical_cols[:5]}...\")\n",
    "    print(\"   Need to encode before SMOTE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1b5d8b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Encode categorical for SMOTE (if any)\n",
    "if categorical_cols:\n",
    "    # One-hot encode categorical columns\n",
    "    X_train_encoded = pd.get_dummies(X_train, columns=categorical_cols, drop_first=True)\n",
    "    X_test_encoded = pd.get_dummies(X_test, columns=categorical_cols, drop_first=True)\n",
    "    \n",
    "    # Align columns (ensure same columns in train and test)\n",
    "    missing_cols = set(X_train_encoded.columns) - set(X_test_encoded.columns)\n",
    "    for col in missing_cols:\n",
    "        X_test_encoded[col] = 0\n",
    "    \n",
    "    extra_cols = set(X_test_encoded.columns) - set(X_train_encoded.columns)\n",
    "    X_test_encoded = X_test_encoded.drop(columns=list(extra_cols))\n",
    "    \n",
    "    # Ensure same column order\n",
    "    X_test_encoded = X_test_encoded[X_train_encoded.columns]\n",
    "    \n",
    "    print(f\"‚úÖ Encoded shapes:\")\n",
    "    print(f\"   X_train_encoded: {X_train_encoded.shape}\")\n",
    "    print(f\"   X_test_encoded: {X_test_encoded.shape}\")\n",
    "else:\n",
    "    X_train_encoded = X_train\n",
    "    X_test_encoded = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a1eaa1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply SMOTE if available\n",
    "if SMOTE_AVAILABLE:\n",
    "    print(\"üîÑ Applying SMOTE...\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    # Drop non-numeric columns for SMOTE\n",
    "    X_train_numeric = X_train_encoded.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Handle any remaining missing values\n",
    "    X_train_numeric = X_train_numeric.fillna(X_train_numeric.median())\n",
    "    \n",
    "    smote = SMOTE(random_state=SEED)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_numeric, y_train)\n",
    "    \n",
    "    print(f\"\\nüìä Before SMOTE:\")\n",
    "    print(f\"   X_train: {X_train_numeric.shape}\")\n",
    "    print(f\"   Class 0: {(y_train == 0).sum():,}\")\n",
    "    print(f\"   Class 1: {(y_train == 1).sum():,}\")\n",
    "    \n",
    "    print(f\"\\nüìä After SMOTE:\")\n",
    "    print(f\"   X_train_resampled: {X_train_resampled.shape}\")\n",
    "    print(f\"   Class 0: {(y_train_resampled == 0).sum():,}\")\n",
    "    print(f\"   Class 1: {(y_train_resampled == 1).sum():,}\")\n",
    "else:\n",
    "    X_train_resampled = X_train_encoded.select_dtypes(include=[np.number])\n",
    "    y_train_resampled = y_train\n",
    "    print(\"‚ö†Ô∏è SMOTE skipped. Using original data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b83eb1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 9. Chu·∫©n B·ªã Data cho Association Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90869a20",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare data for association rules\n",
    "df_association = prepare_for_association_rules(df_features, verbose=True)\n",
    "\n",
    "print(f\"\\nüìä Association Rules Data:\")\n",
    "print(f\"   Shape: {df_association.shape}\")\n",
    "print(f\"   Sample columns: {list(df_association.columns)[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e771970",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 10. L∆∞u D·ªØ Li·ªáu ƒê√£ X·ª≠ L√Ω"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e5ebaa",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create processed data directory\n",
    "processed_dir = project_root / 'data' / 'processed'\n",
    "processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Saving processed data to: {processed_dir}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6efdc7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save all datasets\n",
    "\n",
    "# 1. Full processed data (with features, before encoding)\n",
    "df_features.to_csv(processed_dir / 'hotel_bookings_processed.csv', index=False)\n",
    "print(f\"‚úÖ Saved: hotel_bookings_processed.csv ({df_features.shape})\")\n",
    "\n",
    "# 2. Train set (original)\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "train_df.to_csv(processed_dir / 'train.csv', index=False)\n",
    "print(f\"‚úÖ Saved: train.csv ({train_df.shape})\")\n",
    "\n",
    "# 3. Test set (original)\n",
    "test_df = pd.concat([X_test, y_test], axis=1)\n",
    "test_df.to_csv(processed_dir / 'test.csv', index=False)\n",
    "print(f\"‚úÖ Saved: test.csv ({test_df.shape})\")\n",
    "\n",
    "# 4. Train set encoded (for modeling)\n",
    "X_train_encoded.to_csv(processed_dir / 'X_train_encoded.csv', index=False)\n",
    "y_train.to_csv(processed_dir / 'y_train.csv', index=False)\n",
    "print(f\"‚úÖ Saved: X_train_encoded.csv ({X_train_encoded.shape})\")\n",
    "print(f\"‚úÖ Saved: y_train.csv ({y_train.shape})\")\n",
    "\n",
    "# 5. Test set encoded (for modeling)\n",
    "X_test_encoded.to_csv(processed_dir / 'X_test_encoded.csv', index=False)\n",
    "y_test.to_csv(processed_dir / 'y_test.csv', index=False)\n",
    "print(f\"‚úÖ Saved: X_test_encoded.csv ({X_test_encoded.shape})\")\n",
    "print(f\"‚úÖ Saved: y_test.csv ({y_test.shape})\")\n",
    "\n",
    "# 6. Resampled training data (if SMOTE was applied)\n",
    "if SMOTE_AVAILABLE:\n",
    "    X_train_resampled.to_csv(processed_dir / 'X_train_resampled.csv', index=False)\n",
    "    pd.Series(y_train_resampled, name=TARGET).to_csv(processed_dir / 'y_train_resampled.csv', index=False)\n",
    "    print(f\"‚úÖ Saved: X_train_resampled.csv ({X_train_resampled.shape})\")\n",
    "    print(f\"‚úÖ Saved: y_train_resampled.csv ({len(y_train_resampled)},)\")\n",
    "\n",
    "# 7. Association rules data\n",
    "df_association.to_csv(processed_dir / 'association_rules_data.csv', index=False)\n",
    "print(f\"‚úÖ Saved: association_rules_data.csv ({df_association.shape})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc87cb56",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 11. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ef535a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìù SUMMARY - TI·ªÄN X·ª¨ L√ù & FEATURE ENGINEERING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n‚úÖ TI·ªÄN X·ª¨ L√ù HO√ÄN TH√ÄNH:\")\n",
    "print(f\"   1. Lo·∫°i b·ªè Data Leakage columns: {list(removed_cols & set(leakage_cols))}\")\n",
    "print(f\"   2. X·ª≠ l√Ω Missing values: {missing_total:,} ‚Üí 0\")\n",
    "print(f\"   3. X·ª≠ l√Ω Outliers: IQR method v·ªõi capping\")\n",
    "\n",
    "print(f\"\\n‚úÖ FEATURE ENGINEERING:\")\n",
    "print(f\"   - Columns ban ƒë·∫ßu: {df_raw.shape[1]}\")\n",
    "print(f\"   - Columns sau x·ª≠ l√Ω: {df_features.shape[1]}\")\n",
    "print(f\"   - Features m·ªõi: {len(added_cols)}\")\n",
    "\n",
    "print(f\"\\n‚úÖ TRAIN/TEST SPLIT:\")\n",
    "print(f\"   - Train: {len(y_train):,} samples ({(1-TEST_SIZE)*100:.0f}%)\")\n",
    "print(f\"   - Test: {len(y_test):,} samples ({TEST_SIZE*100:.0f}%)\")\n",
    "\n",
    "if SMOTE_AVAILABLE:\n",
    "    print(f\"\\n‚úÖ SMOTE RESAMPLING:\")\n",
    "    print(f\"   - Before: {len(y_train):,} samples\")\n",
    "    print(f\"   - After: {len(y_train_resampled):,} samples\")\n",
    "\n",
    "print(f\"\\n‚úÖ FILES ƒê√É L∆ØU:\")\n",
    "for f in processed_dir.glob('*.csv'):\n",
    "    size_mb = f.stat().st_size / 1024**2\n",
    "    print(f\"   - {f.name}: {size_mb:.2f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéâ NOTEBOOK HO√ÄN TH√ÄNH!\")\n",
    "print(\"   Ti·∫øp theo: 03_mining_or_clustering.ipynb\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6.204525,
   "end_time": "2026-01-20T01:29:50.695356",
   "environment_variables": {},
   "exception": true,
   "input_path": "C:\\Coding\\DataMining\\Nhom12_BaiTapLon_DataMining\\notebooks\\02_preprocess_feature.ipynb",
   "output_path": "C:\\Coding\\DataMining\\Nhom12_BaiTapLon_DataMining\\outputs\\executed_notebooks\\02_preprocess_feature_20260120_082944.ipynb",
   "parameters": {},
   "start_time": "2026-01-20T01:29:44.490831",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}